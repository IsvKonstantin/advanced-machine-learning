{"nbformat":4,"nbformat_minor":5,"metadata":{"colab":{"name":"transformer.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9fbd4d64","executionInfo":{"status":"ok","timestamp":1634753307705,"user_tz":-180,"elapsed":46259,"user":{"displayName":"Константин Исаев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiD77ZK3rvwFWAwSNtBDWdgXaeYH6JljCw4YqTm=s64","userId":"06419460273569736734"}},"outputId":"41dd972e-23c2-4fb2-c7d8-38363694b01d"},"source":["import re\n","import torch\n","import copy\n","import math\n","import time\n","import torch.nn as nn\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"id":"9fbd4d64","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"c1e935db"},"source":["## Reading, preprocessing and encoding text"],"id":"c1e935db"},{"cell_type":"code","metadata":{"id":"3602832c","executionInfo":{"status":"ok","timestamp":1634753309469,"user_tz":-180,"elapsed":247,"user":{"displayName":"Константин Исаев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiD77ZK3rvwFWAwSNtBDWdgXaeYH6JljCw4YqTm=s64","userId":"06419460273569736734"}}},"source":["class Vocab:\n","    \"\"\"\n","    Simple vocabulary implementation.\n","    \n","    vocab = Vocab(text)\n","    vocab(index) -> Character\n","    vocab[Character] -> index\n","    \"\"\"\n","    def __init__(self, text):\n","        self.symbols  = set(text)\n","        self.c2i = { c : i for i, c in enumerate(self.symbols) }\n","        self.i2c = { i : c for i, c in enumerate(self.symbols) }\n","\n","    def __getitem__(self, char):\n","        return self.c2i[char]\n","\n","    def __char_by_index__(self, index):\n","        return self.i2c[index]\n","\n","    def __len__(self):\n","        return len(self.i2c)\n","\n","    __call__ = __char_by_index__\n","\n","\n","class TextDataset:\n","    \"\"\"\n","    Helper class for working with raw input text.\n","    Has methods for removing unnecessary characters and for encoding characters.\n","    \"\"\"\n","    def __init__(self, sentences):\n","        self._replacements = {\n","            'NL' : lambda x: re.compile('\\n').sub('', x),\n","            '?'  : lambda x: re.compile('\\?').sub('.', x),\n","            '!'  : lambda x: re.compile('!').sub('.', x),\n","            '('  : lambda x: re.compile('\\(').sub('', x),\n","            ')'  : lambda x: re.compile('\\)').sub('', x),\n","            '\"'  : lambda x: re.compile('\"').sub('', x),\n","            '-'  : lambda x: re.compile('\\-').sub(' ', x),\n","            ';'  : lambda x: re.compile(';').sub('.', x),\n","            'DS'  : lambda x: re.compile(r'[\\.]+').sub('.', x)\n","        }\n","        self.replace_bad_chars = lambda x: re.compile(r'[^а-я,\\.\\s]+').sub('', x)\n","        self.text   = self.preprocess(sentences)\n","        self.vocab  = Vocab(self.text)\n","\n","    def replace(self, sentence):\n","        for replacement in self._replacements.values():\n","            sentence = replacement(sentence)\n","\n","        return sentence\n","    \n","    def preprocess(self, sentences):\n","        \"\"\"\n","        Preprocesses text: changes text register to lower case, removes newline characters,\n","        removes non-alphabet characters, translates punctuation marks to dots and commas.\n","        \"\"\"\n","        text = map(self.replace, sentences)\n","        text = map(str.lower, text)\n","        text = map(self.replace_bad_chars, text)\n","        text = map(str.strip, text)\n","        text = filter(lambda t: t, text)\n","        text = ' '.join(text)\n","        return text\n","\n","    @property\n","    def vocab_size(self):\n","        \"\"\"\n","        Returns length of associated vocab\n","        \"\"\"\n","        return len(self.vocab)\n","\n","    def encode(self):\n","        \"\"\"\n","        Encodes processed text using pre-built vocabulary.\n","        \"\"\"\n","        self.text = torch.tensor([self.vocab[char] for char in self.text])"],"id":"3602832c","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"43b8a6ed","executionInfo":{"status":"ok","timestamp":1634753312292,"user_tz":-180,"elapsed":1,"user":{"displayName":"Константин Исаев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiD77ZK3rvwFWAwSNtBDWdgXaeYH6JljCw4YqTm=s64","userId":"06419460273569736734"}}},"source":["def read(path):\n","    with open(path, 'r', encoding='utf8') as f:\n","        return f.readlines()"],"id":"43b8a6ed","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c6f3dda5"},"source":["## Batching data\n","\n","Given a 1-D vector of sequential data, ``batchify()`` arranges the data\n","into ``batch_size`` columns. If the data does not divide evenly into\n","``batch_size`` columns, then the data is trimmed to fit.\n","\n","\\begin{align}\\begin{bmatrix}\n","  \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n","  \\end{bmatrix}\n","  \\Rightarrow\n","  \\begin{bmatrix}\n","  \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n","  \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n","  \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n","  \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n","  \\end{bmatrix}\\end{align}\n","\n","```get_batch()``` generates a pair of input-target sequences for the transformer model. It subdivides the source data into chunks of length bptt.\n","\n","\\begin{align}\n","  \\begin{bmatrix}\n","  \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n","  \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n","  \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n","  \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n","  \\end{bmatrix}\n","  \\Rightarrow\n","  \\begin{bmatrix}\n","  \\begin{bmatrix}\\text{A} & \\text{G} & \\text{M} & \\text{S}\\end{bmatrix} \\\\\n","  \\begin{bmatrix}\\text{B} & \\text{H} & \\text{N} & \\text{T}\\end{bmatrix}\n","  \\end{bmatrix}\n","  \\begin{bmatrix}\n","  \\begin{bmatrix}\\text{B} & \\text{H} & \\text{N} & \\text{T}\\end{bmatrix} \\\\\n","  \\begin{bmatrix}\\text{C} & \\text{I} & \\text{O} & \\text{U}\\end{bmatrix}\n","  \\end{bmatrix}\n","  \\end{align}"],"id":"c6f3dda5"},{"cell_type":"code","metadata":{"id":"b1ff6a56","executionInfo":{"status":"ok","timestamp":1634753314144,"user_tz":-180,"elapsed":347,"user":{"displayName":"Константин Исаев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiD77ZK3rvwFWAwSNtBDWdgXaeYH6JljCw4YqTm=s64","userId":"06419460273569736734"}}},"source":["def batchify(data, bsz):\n","    \"\"\"\n","    Divides the data into bsz separate sequences, removing extra elements\n","    that wouldn't cleanly fit.\n","\n","    (data: Tensor[N], batch_size) -> (data: Tensor[N // batch_size, batch_size])\n","    \"\"\"\n","    seq_len = data.size(0) // bsz\n","    data = data[:seq_len * bsz]\n","    data = data.view(bsz, seq_len).t().contiguous()\n","    return data\n","\n","bptt = 35\n","def get_batch(source, i):\n","    \"\"\"\n","    (Tensor[full_seq_len, batch_size], int) -> (Tensor[seq_len, batch_size], Tensor[seq_len, batch_size])\n","    \"\"\"\n","    seq_len = min(bptt, len(source) - 1 - i)\n","    data = source[i:i+seq_len]\n","    target = source[i+1:i+1+seq_len].reshape(-1)\n","    return data, target"],"id":"b1ff6a56","execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d162c76f","scrolled":true,"executionInfo":{"status":"ok","timestamp":1634753331297,"user_tz":-180,"elapsed":15511,"user":{"displayName":"Константин Исаев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiD77ZK3rvwFWAwSNtBDWdgXaeYH6JljCw4YqTm=s64","userId":"06419460273569736734"}},"outputId":"33543e72-e0f6-459b-d563-2916fdd15160"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","desktop_path = 'C:/Users/Flexatroid/Desktop/AdvancedML/Transformers/text.txt'\n","drive_path = '/content/drive/MyDrive/AdvancedML/text.txt'\n","raw_text = read(drive_path)\n","\n","textDataset = TextDataset(raw_text)\n","textDataset.encode()\n","\n","batch_size = 20\n","eval_batch_size = 10\n","val_size=0.5\n","n = int(len(textDataset.text) // batch_size * val_size)\n","\n","train_data = batchify(textDataset.text[:-2*n], batch_size).to(device)\n","val_data   = batchify(textDataset.text[-2*n:-n], eval_batch_size).to(device)\n","test_data  = batchify(textDataset.text[-n:], eval_batch_size).to(device)\n","\n","print(train_data.size(), val_data.size(), test_data.size())"],"id":"d162c76f","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([41949, 20]) torch.Size([2207, 10]) torch.Size([2207, 10])\n"]}]},{"cell_type":"markdown","metadata":{"id":"151616c2"},"source":["## Transformer model & Positional encoding"],"id":"151616c2"},{"cell_type":"code","metadata":{"id":"fc755259","executionInfo":{"status":"ok","timestamp":1634753334573,"user_tz":-180,"elapsed":425,"user":{"displayName":"Константин Исаев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiD77ZK3rvwFWAwSNtBDWdgXaeYH6JljCw4YqTm=s64","userId":"06419460273569736734"}}},"source":["class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:x.size(0), :]\n","        return self.dropout(x)\n","\n","class TransformerModel(nn.Module):\n","\n","    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n","        super(TransformerModel, self).__init__()\n","        self.pos_encoder = PositionalEncoding(ninp, dropout)\n","        self.encoder     = TransformerEncoder(\n","            encoder_layer=TransformerEncoderLayer(ninp, nhead, nhid, dropout), \n","            num_layers=nlayers\n","        )\n","        self.embs = nn.Embedding(ntoken, ninp)\n","        self.decoder = nn.Linear(ninp, ntoken)\n","\n","    def init_weights(self):\n","        initrange = 0.1\n","        self.encoder.weight.data.uniform_(-initrange, initrange)\n","        self.decoder.bias.data.zero_()\n","        self.decoder.weight.data.uniform_(-initrange, initrange)\n","\n","    def forward(self, x, src_mask):\n","        x = self.embs(x)\n","        x = self.pos_encoder(x)\n","        x = self.encoder(x, src_mask)\n","        x = self.decoder(x)\n","        return x\n","\n","def generate_square_subsequent_mask(sz):\n","    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"],"id":"fc755259","execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"99c40eb3","executionInfo":{"status":"ok","timestamp":1634753583309,"user_tz":-180,"elapsed":220,"user":{"displayName":"Константин Исаев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiD77ZK3rvwFWAwSNtBDWdgXaeYH6JljCw4YqTm=s64","userId":"06419460273569736734"}}},"source":["ntokens = textDataset.vocab_size \n","# emsize  = 512 \n","# nhid    = 256 \n","# nlayers = 5 \n","# nhead   = 8 \n","# dropout = 0.2\n","emsize = 512  # embedding dimension\n","nhid = 256  # dimension of the feedforward network model in nn.TransformerEncoder\n","nlayers = 5  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n","nhead = 8  # number of heads in nn.MultiheadAttention\n","dropout = 0.2  # dropout probability\n","model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"],"id":"99c40eb3","execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"42b29be7"},"source":["## Model training\n","Using CrossEntropyLoss with the SGD (stochastic gradient descent) optimizer. Learning rate is initially set to 0.0001 and follows a StepLR schedule. During training, nn.utils.clip_grad_norm_ is used to prevent gradients from exploding."],"id":"42b29be7"},{"cell_type":"code","metadata":{"id":"dff62991","executionInfo":{"status":"ok","timestamp":1634752417098,"user_tz":-180,"elapsed":222,"user":{"displayName":"Константин Исаев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiD77ZK3rvwFWAwSNtBDWdgXaeYH6JljCw4YqTm=s64","userId":"06419460273569736734"}}},"source":["criterion = nn.CrossEntropyLoss()\n","lr = 0.0001\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n","\n","def train():\n","    model.train() \n","    total_loss = 0.\n","    start_time = time.time()\n","    src_mask = generate_square_subsequent_mask(bptt).to(device)\n","    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n","        data, targets = get_batch(train_data, i)\n","        optimizer.zero_grad()\n","        if data.size(0) != bptt:\n","            src_mask = generate_square_subsequent_mask(data.size(0)).to(device)\n","        output = model(data, src_mask)\n","        loss = criterion(output.view(-1, ntokens), targets)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        log_interval = 100\n","        if batch % log_interval == 0 and batch > 0:\n","            cur_loss = total_loss / log_interval\n","            elapsed = time.time() - start_time\n","            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n","                  'lr {:5.5f} | ms/batch {:5.2f} | '\n","                  'loss {:5.2f} | ppl {:8.2f}'.format(\n","                    epoch, batch, len(train_data) // bptt, scheduler.get_last_lr()[0],\n","                    elapsed * 1000 / log_interval,\n","                    cur_loss, math.exp(cur_loss)))\n","            total_loss = 0\n","            start_time = time.time()\n","\n","def evaluate(eval_model, data_source):\n","    eval_model.eval()\n","    total_loss = 0.\n","    src_mask = generate_square_subsequent_mask(bptt).to(device)\n","    with torch.no_grad():\n","        for i in range(0, data_source.size(0) - 1, bptt):\n","            data, targets = get_batch(data_source, i)\n","            if data.size(0) != bptt:\n","                src_mask = generate_square_subsequent_mask(data.size(0)).to(device)\n","            output = eval_model(data, src_mask)\n","            output_flat = output.view(-1, ntokens)\n","            total_loss += len(data) * criterion(output_flat, targets).item()\n","    return total_loss / (len(data_source) - 1)"],"id":"dff62991","execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a94c7e8d","executionInfo":{"status":"ok","timestamp":1634752685022,"user_tz":-180,"elapsed":250632,"user":{"displayName":"Константин Исаев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiD77ZK3rvwFWAwSNtBDWdgXaeYH6JljCw4YqTm=s64","userId":"06419460273569736734"}},"outputId":"7426ff6c-c513-49e0-b2c3-7dc9626bbc1e"},"source":["best_val_loss = float(\"inf\")\n","epochs = 5 \n","best_model = None\n","\n","for epoch in range(1, epochs + 1):\n","    epoch_start_time = time.time()\n","    train()\n","    val_loss = evaluate(model, val_data)\n","    print('-' * 89)\n","    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n","          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n","                                     val_loss, math.exp(val_loss)))\n","    print('-' * 89)\n","\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        best_model = model\n","\n","    scheduler.step()\n","\n","test_loss = evaluate(best_model, test_data)\n","test_ppl = math.exp(test_loss)\n","print('=' * 89)\n","print(f'| End of training | test loss {test_loss:5.2f} | '\n","      f'test ppl {test_ppl:8.2f}')\n","print('=' * 89)"],"id":"a94c7e8d","execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["| epoch   1 |   100/ 1198 batches | lr 0.00010 | ms/batch 43.82 | loss  2.83 | ppl    16.99\n","| epoch   1 |   200/ 1198 batches | lr 0.00010 | ms/batch 41.59 | loss  2.57 | ppl    13.01\n","| epoch   1 |   300/ 1198 batches | lr 0.00010 | ms/batch 41.68 | loss  2.51 | ppl    12.34\n","| epoch   1 |   400/ 1198 batches | lr 0.00010 | ms/batch 41.46 | loss  2.45 | ppl    11.62\n","| epoch   1 |   500/ 1198 batches | lr 0.00010 | ms/batch 42.03 | loss  2.41 | ppl    11.15\n","| epoch   1 |   600/ 1198 batches | lr 0.00010 | ms/batch 42.11 | loss  2.36 | ppl    10.54\n","| epoch   1 |   700/ 1198 batches | lr 0.00010 | ms/batch 41.28 | loss  2.30 | ppl    10.00\n","| epoch   1 |   800/ 1198 batches | lr 0.00010 | ms/batch 41.51 | loss  2.28 | ppl     9.79\n","| epoch   1 |   900/ 1198 batches | lr 0.00010 | ms/batch 41.96 | loss  2.24 | ppl     9.39\n","| epoch   1 |  1000/ 1198 batches | lr 0.00010 | ms/batch 41.76 | loss  2.20 | ppl     9.05\n","| epoch   1 |  1100/ 1198 batches | lr 0.00010 | ms/batch 41.51 | loss  2.18 | ppl     8.85\n","-----------------------------------------------------------------------------------------\n","| end of epoch   1 | time: 50.55s | valid loss  2.05 | valid ppl     7.77\n","-----------------------------------------------------------------------------------------\n","| epoch   2 |   100/ 1198 batches | lr 0.00009 | ms/batch 42.23 | loss  2.18 | ppl     8.80\n","| epoch   2 |   200/ 1198 batches | lr 0.00009 | ms/batch 41.61 | loss  2.14 | ppl     8.49\n","| epoch   2 |   300/ 1198 batches | lr 0.00009 | ms/batch 41.67 | loss  2.12 | ppl     8.37\n","| epoch   2 |   400/ 1198 batches | lr 0.00009 | ms/batch 41.45 | loss  2.10 | ppl     8.14\n","| epoch   2 |   500/ 1198 batches | lr 0.00009 | ms/batch 41.11 | loss  2.11 | ppl     8.26\n","| epoch   2 |   600/ 1198 batches | lr 0.00009 | ms/batch 41.40 | loss  2.09 | ppl     8.08\n","| epoch   2 |   700/ 1198 batches | lr 0.00009 | ms/batch 41.32 | loss  2.05 | ppl     7.74\n","| epoch   2 |   800/ 1198 batches | lr 0.00009 | ms/batch 41.24 | loss  2.05 | ppl     7.73\n","| epoch   2 |   900/ 1198 batches | lr 0.00009 | ms/batch 41.18 | loss  2.03 | ppl     7.63\n","| epoch   2 |  1000/ 1198 batches | lr 0.00009 | ms/batch 41.23 | loss  2.01 | ppl     7.44\n","| epoch   2 |  1100/ 1198 batches | lr 0.00009 | ms/batch 41.36 | loss  1.99 | ppl     7.35\n","-----------------------------------------------------------------------------------------\n","| end of epoch   2 | time: 50.03s | valid loss  1.86 | valid ppl     6.45\n","-----------------------------------------------------------------------------------------\n","| epoch   3 |   100/ 1198 batches | lr 0.00008 | ms/batch 41.70 | loss  2.02 | ppl     7.53\n","| epoch   3 |   200/ 1198 batches | lr 0.00008 | ms/batch 41.05 | loss  1.99 | ppl     7.31\n","| epoch   3 |   300/ 1198 batches | lr 0.00008 | ms/batch 41.49 | loss  1.98 | ppl     7.27\n","| epoch   3 |   400/ 1198 batches | lr 0.00008 | ms/batch 41.09 | loss  1.97 | ppl     7.16\n","| epoch   3 |   500/ 1198 batches | lr 0.00008 | ms/batch 41.07 | loss  1.99 | ppl     7.34\n","| epoch   3 |   600/ 1198 batches | lr 0.00008 | ms/batch 41.37 | loss  1.98 | ppl     7.22\n","| epoch   3 |   700/ 1198 batches | lr 0.00008 | ms/batch 41.25 | loss  1.93 | ppl     6.91\n","| epoch   3 |   800/ 1198 batches | lr 0.00008 | ms/batch 41.12 | loss  1.94 | ppl     6.97\n","| epoch   3 |   900/ 1198 batches | lr 0.00008 | ms/batch 41.13 | loss  1.93 | ppl     6.87\n","| epoch   3 |  1000/ 1198 batches | lr 0.00008 | ms/batch 41.28 | loss  1.91 | ppl     6.75\n","| epoch   3 |  1100/ 1198 batches | lr 0.00008 | ms/batch 41.15 | loss  1.90 | ppl     6.66\n","-----------------------------------------------------------------------------------------\n","| end of epoch   3 | time: 49.81s | valid loss  1.77 | valid ppl     5.88\n","-----------------------------------------------------------------------------------------\n","| epoch   4 |   100/ 1198 batches | lr 0.00007 | ms/batch 41.37 | loss  1.93 | ppl     6.91\n","| epoch   4 |   200/ 1198 batches | lr 0.00007 | ms/batch 41.31 | loss  1.90 | ppl     6.72\n","| epoch   4 |   300/ 1198 batches | lr 0.00007 | ms/batch 41.06 | loss  1.91 | ppl     6.74\n","| epoch   4 |   400/ 1198 batches | lr 0.00007 | ms/batch 41.27 | loss  1.89 | ppl     6.64\n","| epoch   4 |   500/ 1198 batches | lr 0.00007 | ms/batch 41.01 | loss  1.92 | ppl     6.82\n","| epoch   4 |   600/ 1198 batches | lr 0.00007 | ms/batch 41.01 | loss  1.90 | ppl     6.71\n","| epoch   4 |   700/ 1198 batches | lr 0.00007 | ms/batch 41.02 | loss  1.86 | ppl     6.43\n","| epoch   4 |   800/ 1198 batches | lr 0.00007 | ms/batch 41.31 | loss  1.87 | ppl     6.50\n","| epoch   4 |   900/ 1198 batches | lr 0.00007 | ms/batch 41.15 | loss  1.87 | ppl     6.46\n","| epoch   4 |  1000/ 1198 batches | lr 0.00007 | ms/batch 41.23 | loss  1.84 | ppl     6.31\n","| epoch   4 |  1100/ 1198 batches | lr 0.00007 | ms/batch 41.13 | loss  1.83 | ppl     6.25\n","-----------------------------------------------------------------------------------------\n","| end of epoch   4 | time: 49.71s | valid loss  1.71 | valid ppl     5.54\n","-----------------------------------------------------------------------------------------\n","| epoch   5 |   100/ 1198 batches | lr 0.00007 | ms/batch 41.70 | loss  1.87 | ppl     6.49\n","| epoch   5 |   200/ 1198 batches | lr 0.00007 | ms/batch 41.11 | loss  1.85 | ppl     6.36\n","| epoch   5 |   300/ 1198 batches | lr 0.00007 | ms/batch 41.31 | loss  1.85 | ppl     6.36\n","| epoch   5 |   400/ 1198 batches | lr 0.00007 | ms/batch 40.95 | loss  1.84 | ppl     6.31\n","| epoch   5 |   500/ 1198 batches | lr 0.00007 | ms/batch 41.12 | loss  1.87 | ppl     6.48\n","| epoch   5 |   600/ 1198 batches | lr 0.00007 | ms/batch 41.29 | loss  1.86 | ppl     6.42\n","| epoch   5 |   700/ 1198 batches | lr 0.00007 | ms/batch 41.42 | loss  1.82 | ppl     6.14\n","| epoch   5 |   800/ 1198 batches | lr 0.00007 | ms/batch 41.37 | loss  1.82 | ppl     6.20\n","| epoch   5 |   900/ 1198 batches | lr 0.00007 | ms/batch 41.23 | loss  1.82 | ppl     6.15\n","| epoch   5 |  1000/ 1198 batches | lr 0.00007 | ms/batch 41.13 | loss  1.80 | ppl     6.04\n","| epoch   5 |  1100/ 1198 batches | lr 0.00007 | ms/batch 40.76 | loss  1.79 | ppl     5.99\n","-----------------------------------------------------------------------------------------\n","| end of epoch   5 | time: 49.73s | valid loss  1.67 | valid ppl     5.33\n","-----------------------------------------------------------------------------------------\n","=========================================================================================\n","| End of training | test loss  1.65 | test ppl     5.18\n","=========================================================================================\n"]}]},{"cell_type":"markdown","metadata":{"id":"8556f07d"},"source":["  ## Text generation examples"],"id":"8556f07d"},{"cell_type":"code","metadata":{"id":"4695b858","executionInfo":{"status":"ok","timestamp":1634753352921,"user_tz":-180,"elapsed":211,"user":{"displayName":"Константин Исаев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiD77ZK3rvwFWAwSNtBDWdgXaeYH6JljCw4YqTm=s64","userId":"06419460273569736734"}}},"source":["def gen_char(text, temperature=0):\n","    d = torch.tensor([textDataset.vocab[c] for c in text]).unsqueeze(1).to(device)\n","    m = generate_square_subsequent_mask(len(text)).to(device)\n","    with torch.no_grad():\n","        outputs = best_model(d, m)[-1][0]\n","        if temperature != 0:\n","            outputs = nn.Softmax(0)(outputs / temperature)\n","            idx = torch.multinomial(outputs, 1).item()\n","        else:\n","            idx = outputs.argmax().item()\n","        return textDataset.vocab(idx)\n","\n","def generate(text, n, temperature=0):\n","    for _ in range(n):\n","        text += gen_char(text[-eval_batch_size:], temperature)\n","\n","    return text"],"id":"4695b858","execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0sc0N2G0yl8M","executionInfo":{"status":"ok","timestamp":1634753603116,"user_tz":-180,"elapsed":286,"user":{"displayName":"Константин Исаев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiD77ZK3rvwFWAwSNtBDWdgXaeYH6JljCw4YqTm=s64","userId":"06419460273569736734"}},"outputId":"ffacefa2-541a-4ec0-8ab1-1ddb3cec52eb"},"source":["best_model = model.eval()\n","best_model.load_state_dict(torch.load('/content/drive/MyDrive/AdvancedML/tf_model_final.pt'))  "],"id":"0sc0N2G0yl8M","execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"8B_kL9eX06m3","executionInfo":{"status":"ok","timestamp":1634753605509,"user_tz":-180,"elapsed":1307,"user":{"displayName":"Константин Исаев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiD77ZK3rvwFWAwSNtBDWdgXaeYH6JljCw4YqTm=s64","userId":"06419460273569736734"}},"outputId":"e1603201-d4bf-4943-bf3d-74414ad117e0"},"source":["generate('фродо взял кол', 200, 0)"],"id":"8B_kL9eX06m3","execution_count":17,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'фродо взял колзжпогкдхжхкжзбфх злзй,покчхжхкжзбфх злзй,покчхжхкжзбфх злзй,покчхжхкжзбфх злзй,покчхжхкжзбфх злзй,покчхжхкжзбфх злзй,покчхжхкжзбфх злзй,покчхжхкжзбфх злзй,покчхжхкжзбфх злзй,покчхжхкжзбфх злзй,покчхжх'"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"gxyPMLedt3SI","executionInfo":{"status":"ok","timestamp":1632854984191,"user_tz":-180,"elapsed":1147,"user":{"displayName":"Константин Исаев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiD77ZK3rvwFWAwSNtBDWdgXaeYH6JljCw4YqTm=s64","userId":"06419460273569736734"}},"outputId":"e07ff203-511b-405d-9ea0-f7f0069d6270"},"source":["generate('фродо взял кол', 200, 0.3)"],"id":"gxyPMLedt3SI","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'фродо взял кольцо. я не возвращался и под в нем положил на восток с ним не было от было сображил старый своим своем становился на деревья по не последний в сторону и обеспокойной в от поднялся в стальный собой с по'"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"id":"gb6W_Lr-vBgF","executionInfo":{"status":"ok","timestamp":1632854952967,"user_tz":-180,"elapsed":2124,"user":{"displayName":"Константин Исаев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiD77ZK3rvwFWAwSNtBDWdgXaeYH6JljCw4YqTm=s64","userId":"06419460273569736734"}},"outputId":"d5fe7618-daf6-441a-a308-306a87cf0189"},"source":["generate('гендальф побежал', 400, 0.3)"],"id":"gb6W_Lr-vBgF","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'гендальф побежал он пригорье. я не подняли на кольцо все собой собой страннных в подолжны вернули в помощь на не по от вернулся в под ними, что они поднимали и по не собой строными не поднялась в этом старый в от не собраться и показался в собой в западной воды. они поднялись в собой собенной в последников следующий в пришли по видели на следующий пришли и на стал он по высокой старый последники на запад не могут'"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"MMA3V6-6vdQD","executionInfo":{"status":"ok","timestamp":1634753201166,"user_tz":-180,"elapsed":229,"user":{"displayName":"Константин Исаев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiD77ZK3rvwFWAwSNtBDWdgXaeYH6JljCw4YqTm=s64","userId":"06419460273569736734"}}},"source":["torch.save(best_model.state_dict(), '/content/drive/MyDrive/AdvancedML/tf_model_final.pt')  "],"id":"MMA3V6-6vdQD","execution_count":40,"outputs":[]}]}